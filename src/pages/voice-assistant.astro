---
// Voice Assistant Web Interface
// Uses browser's Web Speech API for speech recognition
// Connects to AI-powered backend via API

import App from "../components/common/App.astro";
import { checkAuth } from "../lib/auth";

const { currentUser } = await checkAuth(Astro.cookies);
---

<App title="Voice Assistant" currentUser={currentUser}>
  <div class="mx-auto max-w-4xl px-4 py-8 sm:px-6 lg:px-8">
    <div class="mb-8 text-center">
      <h1 class="text-4xl font-bold text-gray-900 dark:text-white mb-2">
        üé§ Voice Assistant
      </h1>
      <p class="text-gray-600 dark:text-gray-400">
        Your AI-powered personal assistant powered by Claude AI and Supabase learning tables
      </p>
    </div>

    <div class="rounded-lg border border-gray-200 bg-white shadow-lg dark:border-gray-700 dark:bg-gray-800 p-8">
      <!-- Status Display -->
      <div id="status" class="mb-6 text-center">
        <div class="inline-flex items-center gap-2 px-4 py-2 rounded-full bg-gray-100 dark:bg-gray-700">
          <div id="status-indicator" class="w-3 h-3 rounded-full bg-gray-400"></div>
          <span id="status-text" class="text-sm font-medium text-gray-700 dark:text-gray-300">
            Ready - Click microphone to start
          </span>
        </div>
      </div>

      <!-- Microphone Button -->
      <div class="flex justify-center mb-8">
        <button
          id="mic-button"
          class="w-24 h-24 rounded-full bg-primary-600 hover:bg-primary-700 focus:outline-none focus:ring-4 focus:ring-primary-300 dark:focus:ring-primary-800 transition-all duration-200 flex items-center justify-center shadow-lg hover:shadow-xl"
          aria-label="Start/Stop listening"
        >
          <svg
            id="mic-icon"
            class="w-12 h-12 text-white"
            fill="none"
            stroke="currentColor"
            viewBox="0 0 24 24"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"
            />
          </svg>
        </button>
      </div>

      <!-- Wake Word Input -->
      <div class="mb-6">
        <label for="wake-word" class="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2">
          Wake Word
        </label>
        <input
          type="text"
          id="wake-word"
          value="Bee"
          class="w-full px-4 py-2 border border-gray-300 rounded-md dark:border-gray-600 dark:bg-gray-700 dark:text-white focus:ring-2 focus:ring-primary-500 focus:border-primary-500"
          placeholder="Bee"
        />
        <p class="mt-2 text-xs text-gray-500 dark:text-gray-400">
          Special commands: "Bee stop" (stop talking), "Bee remember this" (save conversation), "Bee new job" (create project)
        </p>
      </div>

      <!-- Conversation Display -->
      <div
        id="conversation"
        class="mb-6 max-h-96 overflow-y-auto space-y-4 p-4 bg-gray-50 dark:bg-gray-900 rounded-lg"
      >
        <div class="text-sm text-gray-500 dark:text-gray-400 text-center">
          Your conversation will appear here...
        </div>
      </div>

      <!-- Tags Editor (shown when ready to save) -->
      <div id="tags-editor" class="mb-6 hidden">
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
          <div>
            <label class="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2">
              Tags (for saving conversation)
            </label>
            <div id="tags-container" class="flex flex-wrap gap-2 mb-2 p-3 bg-gray-50 dark:bg-gray-800 rounded-lg min-h-[3rem]">
              <!-- Tags will be dynamically added here -->
            </div>
            <div class="flex gap-2">
              <input
                type="text"
                id="tag-input"
                placeholder="Add a tag..."
                class="flex-1 px-3 py-2 border border-gray-300 rounded-md dark:border-gray-600 dark:bg-gray-700 dark:text-white focus:ring-2 focus:ring-primary-500 focus:border-primary-500 text-sm"
              />
              <button
                id="add-tag-btn"
                class="px-4 py-2 bg-primary-600 hover:bg-primary-700 text-white rounded-md text-sm font-medium transition-colors"
              >
                Add Tag
              </button>
            </div>
            <p class="mt-2 text-xs text-gray-500 dark:text-gray-400">
              Tags help organize and search saved conversations. Click √ó to remove a tag.
            </p>
          </div>
          <div>
            <label for="priority-select" class="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2">
              Priority
            </label>
            <select
              id="priority-select"
              class="w-full px-3 py-2 border border-gray-300 rounded-md dark:border-gray-600 dark:bg-gray-700 dark:text-white focus:ring-2 focus:ring-primary-500 focus:border-primary-500 text-sm"
            >
              <option value="0">Normal (0)</option>
              <option value="1">Low (1)</option>
              <option value="5">Medium (5)</option>
              <option value="10">High (10)</option>
              <option value="20">Critical (20)</option>
            </select>
            <p class="mt-2 text-xs text-gray-500 dark:text-gray-400">
              Higher priority entries appear first in search results.
            </p>
          </div>
        </div>
      </div>

      <!-- Quick Action Buttons -->
      <div class="mb-6">
        <p class="text-sm font-medium text-gray-700 dark:text-gray-300 mb-3">
          Quick Actions (fallback if voice fails):
        </p>
        <div class="flex flex-wrap gap-2">
          <button
            id="btn-stop"
            class="px-4 py-2 bg-red-600 hover:bg-red-700 text-white rounded-md text-sm font-medium transition-colors focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-offset-2"
            title="Stop speaking (Bee stop)"
          >
            üõë Stop Speaking
          </button>
          <button
            id="btn-remember"
            class="px-4 py-2 bg-purple-600 hover:bg-purple-700 text-white rounded-md text-sm font-medium transition-colors focus:outline-none focus:ring-2 focus:ring-purple-500 focus:ring-offset-2"
            title="Save conversation (Bee remember this)"
          >
            üíæ Remember This
          </button>
          <button
            id="btn-help"
            class="px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white rounded-md text-sm font-medium transition-colors focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2"
            title="Show help"
          >
            ‚ùì Help
          </button>
          <button
            id="btn-export-logs"
            class="px-4 py-2 bg-green-600 hover:bg-green-700 text-white rounded-md text-sm font-medium transition-colors focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-offset-2"
            title="Export logs"
          >
            üìã Export Logs (<span id="log-count">0</span>)
          </button>
        </div>
      </div>

      <!-- Settings -->
      <div class="flex items-center justify-between text-sm mb-4">
        <label class="flex items-center gap-2 text-gray-700 dark:text-gray-300">
          <input
            type="checkbox"
            id="enable-speech"
            checked
            class="rounded border-gray-300 text-primary-600 focus:ring-primary-500"
          />
          Enable voice responses
        </label>
        <label class="flex items-center gap-2 text-gray-700 dark:text-gray-300">
          <input
            type="checkbox"
            id="skip-wake-word"
            class="rounded border-gray-300 text-primary-600 focus:ring-primary-500"
          />
          Skip wake word (process all speech)
        </label>
        <label class="flex items-center gap-2 text-gray-700 dark:text-gray-300">
          <input
            type="checkbox"
            id="use-cloud-transcription"
            class="rounded border-gray-300 text-primary-600 focus:ring-primary-500"
          />
          Use cloud transcription (better accuracy)
          <span id="cloud-transcription-status" class="text-xs text-gray-500 dark:text-gray-400 ml-1"></span>
        </label>
        <label class="flex items-center gap-2 text-gray-700 dark:text-gray-300">
          <input
            type="checkbox"
            id="enable-tags"
            class="rounded border-gray-300 text-primary-600 focus:ring-primary-500"
          />
          Enable tag extraction (may affect agent)
        </label>
        <button
          id="clear-conversation"
          class="text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300"
        >
          Clear Conversation
        </button>
      </div>
      
      <!-- Debug Info -->
      <div id="debug-info" class="text-xs text-gray-500 dark:text-gray-400 mt-2 hidden">
        <div>Debug: <span id="debug-text">-</span></div>
      </div>
    </div>

    <!-- Instructions -->
    <div class="mt-8 rounded-lg border border-gray-200 bg-gray-50 dark:border-gray-700 dark:bg-gray-800 p-6">
      <h2 class="text-lg font-semibold text-gray-900 dark:text-white mb-3">How to Use</h2>
      <ol class="list-decimal list-inside space-y-2 text-gray-700 dark:text-gray-300">
        <li>Click the microphone button to start listening</li>
        <li>Say "Bee" followed by your question (e.g., "Bee what is fire protection?")</li>
        <li>The assistant will process your request using AI and respond</li>
        <li>Special commands:
          <ul class="list-disc list-inside ml-4 mt-1">
            <li><strong>"Bee stop"</strong> - Stop the assistant from talking</li>
            <li><strong>"Bee remember this"</strong> - Save the current conversation to memory</li>
            <li><strong>"Bee new job"</strong> or <strong>"Bee new project"</strong> - Create a new project (I'll ask you for all the details sequentially)
              <ul class="list-disc list-inside ml-4 mt-1">
                <li>For Admin/Staff: First asks if you want to use an existing client or create a new one</li>
                <li>For Clients: Starts directly with project details</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>Click the microphone again to stop listening</li>
      </ol>
      <p class="mt-4 text-sm text-gray-600 dark:text-gray-400">
        <strong>Note:</strong> This uses your browser's built-in speech recognition. Works best in Chrome, Edge, or Safari.
        <br />
        <strong>Tip:</strong> Enable "Use cloud transcription" for better accuracy (requires API key configuration).
      </p>
    </div>
  </div>
</App>

<script define:vars={{ currentUserRole: currentUser?.profile?.role || "Client" }}>
  // Voice Assistant Web Interface
  // Uses Web Speech API for speech recognition

  // Get current user info from server
  const isClient = currentUserRole.toLowerCase() === "client";

  const micButton = document.getElementById("mic-button");
  const micIcon = document.getElementById("mic-icon");
  const statusText = document.getElementById("status-text");
  const statusIndicator = document.getElementById("status-indicator");
  const conversation = document.getElementById("conversation");
  const wakeWordInput = document.getElementById("wake-word");
  const enableSpeechCheckbox = document.getElementById("enable-speech");
  const skipWakeWordCheckbox = document.getElementById("skip-wake-word");
  const useCloudTranscriptionCheckbox = document.getElementById("use-cloud-transcription");
  const enableTagsCheckbox = document.getElementById("enable-tags");
  const clearButton = document.getElementById("clear-conversation");
  const btnStop = document.getElementById("btn-stop");
  const btnRemember = document.getElementById("btn-remember");
  const btnHelp = document.getElementById("btn-help");
  const tagsEditor = document.getElementById("tags-editor");
  const tagsContainer = document.getElementById("tags-container");
  const tagInput = document.getElementById("tag-input");
  const addTagBtn = document.getElementById("add-tag-btn");
  const prioritySelect = document.getElementById("priority-select");
  const btnExportLogs = document.getElementById("btn-export-logs");
  const logCountSpan = document.getElementById("log-count");
  
  let currentTags = []; // Store tags for current conversation
  let currentPriority = 0; // Store priority for current conversation
  const debugInfo = document.getElementById("debug-info");
  const debugText = document.getElementById("debug-text");
  
  // Project creation state
  let projectCreationState = null; // null | { step: string, data: object }
  let isAwaitingProjectField = false;
  
  // Debouncing and duplicate prevention
  let lastProcessedTranscript = "";
  let lastProcessedTime = 0;
  let processingCooldown = 2000; // 2 seconds cooldown between processing
  let isProcessingTranscript = false;
  
  // Logging system
  const VoiceAssistantLogger = {
    logs: [],
    maxLogs: 10000, // Maximum logs to keep in memory
    
    init() {
      // Load logs from localStorage on init
      try {
        const savedLogs = localStorage.getItem("voiceAssistantLogs");
        if (savedLogs) {
          this.logs = JSON.parse(savedLogs);
          this.updateLogCount();
        }
      } catch (error) {
        console.error("üé§ [LOGGER] Error loading logs from localStorage:", error);
      }
    },
    
    log(level, category, message, data = null) {
      const logEntry = {
        timestamp: new Date().toISOString(),
        level, // 'info', 'warn', 'error', 'debug'
        category, // 'command', 'project', 'voice', 'api', 'state', etc.
        message,
        data: data ? JSON.parse(JSON.stringify(data)) : null, // Deep clone to avoid reference issues
        userAgent: navigator.userAgent,
        url: window.location.href,
      };
      
      this.logs.push(logEntry);
      
      // Keep only the most recent logs
      if (this.logs.length > this.maxLogs) {
        this.logs = this.logs.slice(-this.maxLogs);
      }
      
      // Save to localStorage
      try {
        localStorage.setItem("voiceAssistantLogs", JSON.stringify(this.logs));
      } catch (error) {
        console.error("üé§ [LOGGER] Error saving logs to localStorage:", error);
        // If localStorage is full, remove oldest logs
        if (error.name === "QuotaExceededError") {
          this.logs = this.logs.slice(-Math.floor(this.maxLogs / 2));
          try {
            localStorage.setItem("voiceAssistantLogs", JSON.stringify(this.logs));
          } catch (e) {
            console.error("üé§ [LOGGER] Failed to save logs after cleanup:", e);
          }
        }
      }
      
      this.updateLogCount();
      
      // Also log to console with prefix
      const consoleMethod = level === "error" ? console.error : level === "warn" ? console.warn : console.log;
      consoleMethod(`üé§ [${category.toUpperCase()}] ${message}`, data || "");
    },
    
    info(category, message, data) {
      this.log("info", category, message, data);
    },
    
    warn(category, message, data) {
      this.log("warn", category, message, data);
    },
    
    error(category, message, data) {
      this.log("error", category, message, data);
    },
    
    debug(category, message, data) {
      this.log("debug", category, message, data);
    },
    
    updateLogCount() {
      if (logCountSpan) {
        logCountSpan.textContent = this.logs.length;
      }
    },
    
    exportLogs() {
      const exportData = {
        exportedAt: new Date().toISOString(),
        totalLogs: this.logs.length,
        logs: this.logs,
        summary: {
          byLevel: {},
          byCategory: {},
          errors: this.logs.filter(l => l.level === "error").length,
          warnings: this.logs.filter(l => l.level === "warn").length,
        },
      };
      
      // Calculate summaries
      this.logs.forEach(log => {
        exportData.summary.byLevel[log.level] = (exportData.summary.byLevel[log.level] || 0) + 1;
        exportData.summary.byCategory[log.category] = (exportData.summary.byCategory[log.category] || 0) + 1;
      });
      
      // Create downloadable file
      const blob = new Blob([JSON.stringify(exportData, null, 2)], { type: "application/json" });
      const url = URL.createObjectURL(blob);
      const a = document.createElement("a");
      a.href = url;
      a.download = `voice-assistant-logs-${new Date().toISOString().replace(/:/g, "-").split(".")[0]}.json`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
      
      this.info("export", "Logs exported", { totalLogs: this.logs.length });
    },
    
    clearLogs() {
      this.logs = [];
      localStorage.removeItem("voiceAssistantLogs");
      this.updateLogCount();
      this.info("export", "Logs cleared");
    },
  };
  
  // Initialize logger
  VoiceAssistantLogger.init();
  VoiceAssistantLogger.info("system", "Voice Assistant initialized", {
    userRole: currentUserRole,
    isClient,
    userAgent: navigator.userAgent,
    tagsEnabled: enableTagsCheckbox?.checked || false,
  });
  
  // Check cloud transcription availability on load
  async function checkCloudTranscriptionAvailability() {
    const statusElement = document.getElementById("cloud-transcription-status");
    if (!statusElement) return;
    
    try {
      // Try a test request to see if transcription is configured
      const testResponse = await fetch("/api/voice-assistant/transcribe", {
        method: "POST",
        body: new FormData(), // Empty form data to test
      });
      
      if (testResponse.status === 400) {
        // 400 means endpoint exists but needs audio - this is good
        statusElement.textContent = "(configured)";
        statusElement.className = "text-xs text-green-600 dark:text-green-400 ml-1";
        VoiceAssistantLogger.info("system", "Cloud transcription is configured");
      } else if (testResponse.status === 500) {
        const errorData = await testResponse.json().catch(() => ({}));
        if (errorData.error?.includes("not configured")) {
          statusElement.textContent = "(not configured)";
          statusElement.className = "text-xs text-yellow-600 dark:text-yellow-400 ml-1";
          VoiceAssistantLogger.warn("system", "Cloud transcription not configured");
          
          // If cloud transcription is checked, uncheck it and show warning
          if (useCloudTranscriptionCheckbox?.checked) {
            useCloudTranscriptionCheckbox.checked = false;
            addMessage("system", "‚ö†Ô∏è Cloud transcription is not configured. Please set DEEPGRAM_API_KEY or GOOGLE_CLOUD_SPEECH_API_KEY in your environment, or use browser speech recognition (currently enabled).", []);
          }
        }
      }
    } catch (error) {
      // Network error or endpoint doesn't exist
      statusElement.textContent = "(checking...)";
      statusElement.className = "text-xs text-gray-500 dark:text-gray-400 ml-1";
      VoiceAssistantLogger.debug("system", "Could not check cloud transcription availability", { error: error.message });
    }
  }
  
  // Check cloud transcription on page load
  checkCloudTranscriptionAvailability();
  
  // Log when tag toggle changes
  if (enableTagsCheckbox) {
    enableTagsCheckbox.addEventListener("change", (e) => {
      const enabled = e.target.checked;
      VoiceAssistantLogger.info("system", "Tag extraction toggled", { enabled });
      // Hide tags editor if disabled
      if (!enabled && tagsEditor) {
        tagsEditor.classList.add("hidden");
        currentTags = [];
        updateTagsDisplay();
      }
    });
  }
  
  // Warn when cloud transcription is enabled but fails
  if (useCloudTranscriptionCheckbox) {
    useCloudTranscriptionCheckbox.addEventListener("change", (e) => {
      const enabled = e.target.checked;
      VoiceAssistantLogger.info("system", "Cloud transcription toggled", { enabled });
      
      if (enabled) {
        // Re-check availability when enabled
        setTimeout(() => {
          checkCloudTranscriptionAvailability();
        }, 500);
      }
    });
  }

  let recognition = null;
  let isListening = false;
  let conversationHistory = [];
  let isAwake = false;
  let wakeWord = "Bee";
  let currentSpeechUtterance = null;
  let mediaRecorder = null;
  let audioChunks = [];
  let audioStream = null;

  // Check for browser support
  const SpeechRecognition =
    window.SpeechRecognition || window.webkitSpeechRecognition;

  if (!SpeechRecognition) {
    statusText.textContent = "Speech recognition not supported in this browser";
    statusIndicator.className = "w-3 h-3 rounded-full bg-red-400";
    micButton.disabled = true;
  } else {
    recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true; // Enable interim results for better accuracy
    recognition.lang = "en-US";
    recognition.maxAlternatives = 3; // Get multiple transcription alternatives
    
    // Add grammar hints for better recognition (if supported)
    try {
      const grammar = `#JSGF V1.0; grammar wakeword; public <wakeword> = Bee | bee | B | b;`;
      const speechRecognitionList = new (window.SpeechGrammarList || window.webkitSpeechGrammarList)();
      speechRecognitionList.addFromString(grammar, 1);
      recognition.grammars = speechRecognitionList;
    } catch (e) {
      // Grammar not supported, continue without it
      console.log("Speech grammar not supported");
    }

    recognition.onstart = () => {
      isListening = true;
      updateStatus("Listening...", "bg-blue-400");
      micButton.classList.add("ring-4", "ring-blue-300");
      VoiceAssistantLogger.info("voice", "Speech recognition started");
    };

    recognition.onend = () => {
      isListening = false;
      updateStatus("Ready - Click microphone to start", "bg-gray-400");
      micButton.classList.remove("ring-4", "ring-blue-300");
      VoiceAssistantLogger.debug("voice", "Speech recognition ended", { isAwake });
      
      // Auto-restart if it stopped unexpectedly (browser sometimes stops recognition)
      // Only restart if we were actively listening
      if (isAwake) {
        setTimeout(() => {
          if (!isListening) {
            try {
              recognition.start();
              VoiceAssistantLogger.debug("voice", "Speech recognition auto-restarted");
            } catch (e) {
              VoiceAssistantLogger.warn("voice", "Recognition already starting", { error: e.message });
            }
          }
        }, 100);
      }
    };

    recognition.onerror = (event) => {
      console.error("Speech recognition error:", event.error);
      VoiceAssistantLogger.error("voice", "Speech recognition error", { error: event.error, event });
      updateStatus(`Error: ${event.error}`, "bg-red-400");
      isListening = false;
    };

    recognition.onresult = async (event) => {
      // If using cloud transcription, skip Web Speech API results
      if (useCloudTranscriptionCheckbox?.checked) {
        return; // Cloud transcription handles this separately
      }
      
      // Prevent duplicate processing
      if (isProcessingTranscript) {
        VoiceAssistantLogger.debug("voice", "Skipping duplicate transcript processing");
        return;
      }
      
      // Get the most confident transcript from FINAL results only
      let transcript = "";
      let confidence = 0;
      let hasFinalResult = false;
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i];
        if (result.isFinal) {
          hasFinalResult = true;
          // Use the most confident alternative
          const alternatives = Array.from(result);
          const bestAlternative = alternatives.reduce((best, current) => {
            return (current.confidence || 0) > (best.confidence || 0) ? current : best;
          });
          
          if (bestAlternative.confidence > confidence) {
            transcript = bestAlternative.transcript;
            confidence = bestAlternative.confidence;
          }
        }
        // Skip interim results - we only want final transcripts
      }
      
      // Only process if we have a final result
      if (!hasFinalResult) {
        VoiceAssistantLogger.debug("voice", "Skipping interim transcript", { 
          hasFinalResult: false 
        });
        return;
      }
      
      transcript = transcript.trim();
      if (!transcript) return;
      
      // Check for duplicate transcript (same as last processed)
      const now = Date.now();
      if (transcript.toLowerCase() === lastProcessedTranscript.toLowerCase() && 
          (now - lastProcessedTime) < processingCooldown) {
        VoiceAssistantLogger.debug("voice", "Skipping duplicate transcript", {
          transcript,
          timeSinceLastProcess: now - lastProcessedTime,
        });
        return;
      }
      
      // Post-process transcript to fix common errors
      transcript = correctCommonErrors(transcript);
      
      // Update last processed
      lastProcessedTranscript = transcript.toLowerCase();
      lastProcessedTime = now;
      isProcessingTranscript = true;
      
      VoiceAssistantLogger.info("voice", "Transcript received (final)", { 
        transcript,
        confidence,
        isFinal: true 
      });
      
      // Process the transcript
      try {
        await processTranscript(transcript);
      } finally {
        // Reset processing flag after a short delay to allow for natural speech pauses
        setTimeout(() => {
          isProcessingTranscript = false;
        }, 1500);
      }
    };
    
    // Process transcript (used by both Web Speech API and cloud transcription)
    async function processTranscript(transcript) {

      const skipWakeWord = skipWakeWordCheckbox.checked;
      const currentWakeWord = wakeWordInput.value.toLowerCase().trim();
      const transcriptLower = transcript.toLowerCase();

      // Check if wake word is in the transcript
      const wakeWordIndex = transcriptLower.indexOf(currentWakeWord);
      const hasWakeWord = wakeWordIndex !== -1;

      // Extract command (remove wake word if present)
      let command = transcript;
      if (hasWakeWord) {
        // Remove wake word and clean up
        command = transcript
          .replace(new RegExp(currentWakeWord, "gi"), "")
          .trim()
          .replace(/\s+/g, " ");
      }

      // Debug logging
      debugText.textContent = `Transcript: "${transcript}" | Command: "${command}" | HasWakeWord: ${hasWakeWord} | SkipWakeWord: ${skipWakeWord}`;
      debugInfo.classList.remove("hidden");

      // Process if: skip wake word mode OR wake word detected OR already awake
      const shouldProcess = skipWakeWord || hasWakeWord || isAwake;

      if (shouldProcess) {
        // Set awake state
        if (!isAwake && (skipWakeWord || hasWakeWord)) {
          isAwake = true;
          updateStatus("Awake - Processing...", "bg-green-400");
        }

        // Process command if there's content (or if skipping wake word)
        if (skipWakeWord ? transcript.length > 0 : command.length > 0) {
          const commandToProcess = skipWakeWord ? transcript : command;
          const commandLower = commandToProcess.toLowerCase().trim();
          
          VoiceAssistantLogger.info("command", "Processing command", {
            command: commandToProcess,
            hasWakeWord,
            skipWakeWord,
            isAwake,
          });
          
          // Extract and show tags for user message (only if enabled)
          let userTags = [];
          if (enableTagsCheckbox?.checked) {
            VoiceAssistantLogger.debug("tags", "Extracting tags from transcript");
            userTags = await extractTags(transcript);
            // Add extracted tags to current tags
            userTags.forEach(tag => addTag(tag));
            // Show tags editor
            if (tagsEditor && userTags.length > 0) {
              tagsEditor.classList.remove("hidden");
            }
          } else {
            VoiceAssistantLogger.debug("tags", "Tag extraction disabled, skipping");
          }
          addMessage("user", transcript, userTags);
          
          // Check for special keyword commands
          if (commandLower.includes("stop") || commandLower === "stop") {
            VoiceAssistantLogger.info("command", "Stop command detected");
            // Stop talking command
            stopSpeaking();
            addMessage("assistant", "Stopped speaking.");
            updateStatus("Ready - Say wake word", "bg-gray-400");
            if (!skipWakeWordCheckbox.checked) {
              setTimeout(() => {
                isAwake = false;
              }, 2000);
            }
            return;
          }
          
          if (commandLower.includes("remember this") || commandLower.includes("remember that")) {
            // Save conversation to learning table
            VoiceAssistantLogger.info("command", "Remember command detected");
            updateStatus("Saving conversation...", "bg-purple-400");
            try {
              const saved = await saveConversation();
              if (saved) {
                VoiceAssistantLogger.info("command", "Conversation saved successfully");
                addMessage("assistant", "I've saved this conversation to my memory.");
                updateStatus("Conversation saved!", "bg-green-400");
              } else {
                VoiceAssistantLogger.warn("command", "Failed to save conversation");
                addMessage("assistant", "I couldn't save the conversation. Please check the console for errors.");
                updateStatus("Save failed", "bg-red-400");
              }
            } catch (error) {
              VoiceAssistantLogger.error("command", "Error saving conversation", { error: error.message, stack: error.stack });
              console.error("Error saving conversation:", error);
              addMessage("assistant", "I encountered an error saving the conversation.");
              updateStatus("Error saving", "bg-red-400");
            }
            
            if (!skipWakeWordCheckbox.checked) {
              setTimeout(() => {
                isAwake = false;
              }, 2000);
            }
            return;
          }
          
          // Handle project field collection if we're in project creation mode (check this first)
          if (isAwaitingProjectField && projectCreationState) {
            // Check if this is a duplicate of the last processed input
            const normalizedInput = commandToProcess.toLowerCase().trim();
            if (normalizedInput === lastProcessedTranscript && 
                (Date.now() - lastProcessedTime) < processingCooldown) {
              VoiceAssistantLogger.debug("project", "Skipping duplicate project field input", {
                input: commandToProcess,
                step: projectCreationState.step,
              });
              return;
            }
            
            VoiceAssistantLogger.debug("project", "Handling project field input", { 
              step: projectCreationState.step,
              input: commandToProcess 
            });
            
            // Update last processed before handling
            lastProcessedTranscript = normalizedInput;
            lastProcessedTime = Date.now();
            
            await handleProjectFieldInput(commandToProcess);
            return;
          }
          
          // Check for "Bee new job" or "Bee new project" command
          if (commandLower.includes("new job") || commandLower.includes("new project")) {
            if (isAwaitingProjectField) {
              VoiceAssistantLogger.warn("project", "Project creation already in progress");
              const response = "I'm already helping you create a project. Please answer my current question, or say 'cancel' to start over.";
              addMessage("assistant", response);
              if (enableSpeechCheckbox.checked) {
                speak(response);
              }
              return;
            }
            
            VoiceAssistantLogger.info("project", "Starting project creation", { isClient, userRole: currentUserRole });
            
            // Start project creation flow
            // Reset processing state to allow new inputs
            lastProcessedTranscript = "";
            lastProcessedTime = 0;
            
            if (!isClient) {
              // Admin/Staff: Ask about client first
              projectCreationState = {
                step: "clientType",
                data: {}
              };
              isAwaitingProjectField = true;
              const response = "I'll help you create a new project. First, do you want to use an existing client or create a new client? Say 'existing' or 'new'.";
              addMessage("assistant", response);
              updateStatus("Creating project - Client selection", "bg-blue-400");
              
              // Wait before speaking to prevent immediate re-processing
              if (enableSpeechCheckbox.checked) {
                setTimeout(() => {
                  speak(response);
                }, 800);
              }
            } else {
              // Client: Start directly with address
              projectCreationState = {
                step: "address",
                data: {}
              };
              isAwaitingProjectField = true;
              const response = "I'll help you create a new project. Let's start with the project address. What's the address for this project?";
              addMessage("assistant", response);
              updateStatus("Creating project - Address", "bg-blue-400");
              
              // Wait before speaking to prevent immediate re-processing
              if (enableSpeechCheckbox.checked) {
                setTimeout(() => {
                  speak(response);
                }, 800);
              }
            }
            return;
          }
          
          // Regular command processing
          updateStatus("Processing...", "bg-yellow-400");

          try {
            VoiceAssistantLogger.info("api", "Processing command via API", { command: commandToProcess });
            console.log("Processing command:", commandToProcess);
            const response = await processCommand(commandToProcess);
            console.log("Received response:", response);
            
            VoiceAssistantLogger.info("api", "Received API response", { 
              responseLength: response?.length,
              hasResponse: !!response 
            });
            
            if (response) {
              addMessage("assistant", response);

              // Speak response if enabled
              if (enableSpeechCheckbox.checked) {
                VoiceAssistantLogger.debug("voice", "Speaking response", { responseLength: response.length });
                speak(response);
              }

              updateStatus("Ready - Say wake word", "bg-gray-400");
            } else {
              throw new Error("Empty response from API");
            }

            // Return to wake word listening after a delay (unless skip mode)
            if (!skipWakeWordCheckbox.checked) {
              setTimeout(() => {
                isAwake = false;
              }, 2000);
            }
          } catch (error) {
            VoiceAssistantLogger.error("api", "Error processing command", { 
              error: error.message, 
              stack: error.stack,
              command: commandToProcess 
            });
            console.error("Error processing command:", error);
            const errorMsg = `Sorry, I encountered an error: ${error.message || "Please try again."}`;
            addMessage("assistant", errorMsg);
            updateStatus("Error - Try again", "bg-red-400");
            if (enableSpeechCheckbox.checked) {
              speak(errorMsg);
            }
            if (!skipWakeWordCheckbox.checked) {
              isAwake = false;
            }
          }
        } else if (hasWakeWord && command.length === 0 && !skipWakeWord) {
          // Wake word detected but no command yet
          isAwake = true;
          // Extract tags even for just wake word (only if enabled)
          let userTags = [];
          if (enableTagsCheckbox?.checked) {
            VoiceAssistantLogger.debug("tags", "Extracting tags from wake word");
            userTags = await extractTags(transcript);
            // Add extracted tags to current tags
            userTags.forEach(tag => addTag(tag));
            // Show tags editor
            if (tagsEditor && userTags.length > 0) {
              tagsEditor.classList.remove("hidden");
            }
          }
          addMessage("user", transcript, userTags);
          updateStatus("Awake - Listening for command...", "bg-green-400");
          // Keep listening - don't return
        }
      }
    };
  }

  // Handle project field input sequentially
  async function handleProjectFieldInput(input) {
    if (!projectCreationState) {
      VoiceAssistantLogger.warn("project", "handleProjectFieldInput called but no projectCreationState");
      return;
    }
    
    VoiceAssistantLogger.info("project", "Processing project field input", {
      step: projectCreationState.step,
      input,
      dataKeys: Object.keys(projectCreationState.data || {}),
    });
    
    // Check for cancellation
    const inputLower = input.toLowerCase().trim();
    if (inputLower === "cancel" || inputLower === "stop" || inputLower.includes("cancel") || inputLower.includes("never mind")) {
      VoiceAssistantLogger.info("project", "Project creation cancelled by user");
      projectCreationState = null;
      isAwaitingProjectField = false;
      const cancelMsg = "Project creation cancelled. You can start again anytime by saying 'Bee new job'.";
      addMessage("assistant", cancelMsg);
      if (enableSpeechCheckbox.checked) {
        speak(cancelMsg);
      }
      updateStatus("Ready - Say wake word", "bg-gray-400");
      if (!skipWakeWordCheckbox.checked) {
        setTimeout(() => {
          isAwake = false;
        }, 2000);
      }
      return;
    }
    
    const { step, data } = projectCreationState;
    let nextStep = null;
    let response = "";
    
    // Process current step and move to next
    switch (step) {
      case "clientType":
        const clientTypeLower = input.toLowerCase().trim();
        VoiceAssistantLogger.debug("project", "Processing clientType input", { input, clientTypeLower });
        
        if (clientTypeLower.includes("existing") || clientTypeLower.includes("current") || clientTypeLower.includes("old")) {
          projectCreationState.clientType = "existing";
          nextStep = "searchClient";
          VoiceAssistantLogger.info("project", "Client type selected: existing", { nextStep });
          response = "Got it. Let's find an existing client. What's the client's name, company name, or email? I'll search for them.";
        } else if (clientTypeLower.includes("new") || clientTypeLower.includes("create")) {
          projectCreationState.clientType = "new";
          nextStep = "firstName";
          VoiceAssistantLogger.info("project", "Client type selected: new", { nextStep });
          response = "Great! Let's create a new client. What's the client's first name?";
        } else {
          // Didn't understand, ask again
          VoiceAssistantLogger.warn("project", "Unclear client type response", { input, clientTypeLower });
          response = "I didn't understand. Do you want to use an existing client or create a new client? Say 'existing' or 'new'.";
          addMessage("assistant", response);
          if (enableSpeechCheckbox.checked) {
            setTimeout(() => {
              speak(response);
            }, 500);
          }
          return; // Return early to prevent state update
        }
        break;
        
      case "searchClient":
        // Search for clients
        const searchResults = await searchClients(input.trim());
        if (searchResults.length === 0) {
          response = `I couldn't find any clients matching "${input}". Would you like to search again with different terms, or create a new client instead? Say 'search again', 'new client', or 'cancel'.`;
          nextStep = "searchClient"; // Stay on search step
        } else if (searchResults.length === 1) {
          // Single match - use it
          data.authorId = searchResults[0].id;
          data.clientName = searchResults[0].companyName || `${searchResults[0].firstName} ${searchResults[0].lastName}`;
          nextStep = "address";
          response = `Found client: ${data.clientName}. Now let's get the project details. What's the address for this project?`;
        } else {
          // Multiple matches - list them and ask to select
          projectCreationState.searchResults = searchResults;
          nextStep = "selectClient";
          const clientList = searchResults.slice(0, 5).map((c, i) => 
            `${i + 1}. ${c.companyName || `${c.firstName} ${c.lastName}`} (${c.email})`
          ).join(". ");
          response = `I found ${searchResults.length} clients. Here are the first few: ${clientList}. Which one do you want? Say the number (1-${Math.min(5, searchResults.length)}) or the company name.`;
        }
        break;
        
      case "selectClient":
        // User selecting from search results
        const selection = input.toLowerCase().trim();
        const numMatch = selection.match(/\d+/);
        let selectedClient = null;
        
        if (numMatch) {
          const index = parseInt(numMatch[0]) - 1;
          if (index >= 0 && index < projectCreationState.searchResults.length) {
            selectedClient = projectCreationState.searchResults[index];
          }
        } else {
          // Try to match by name
          selectedClient = projectCreationState.searchResults.find(c => 
            c.companyName?.toLowerCase().includes(selection) ||
            `${c.firstName} ${c.lastName}`.toLowerCase().includes(selection) ||
            c.email?.toLowerCase().includes(selection)
          );
        }
        
        if (selectedClient) {
          data.authorId = selectedClient.id;
          data.clientName = selectedClient.companyName || `${selectedClient.firstName} ${selectedClient.lastName}`;
          nextStep = "address";
          response = `Selected client: ${data.clientName}. Now let's get the project details. What's the address for this project?`;
        } else {
          response = `I couldn't match that selection. Please say the number (1-${Math.min(5, projectCreationState.searchResults.length)}) or the company name from the list.`;
          nextStep = "selectClient"; // Stay on selection step
        }
        break;
        
      case "firstName":
        data.firstName = input.trim();
        nextStep = "lastName";
        response = `First name: ${data.firstName}. What's the last name?`;
        break;
        
      case "lastName":
        data.lastName = input.trim();
        nextStep = "email";
        response = `Last name: ${data.lastName}. What's the email address?`;
        break;
        
      case "email":
        const emailMatch = input.match(/[\w\.-]+@[\w\.-]+\.\w+/);
        if (emailMatch) {
          data.email = emailMatch[0];
        } else {
          data.email = input.trim();
        }
        nextStep = "companyName";
        response = `Email: ${data.email}. What's the company name? (Say "skip" if not applicable)`;
        break;
        
      case "companyName":
        if (!input.toLowerCase().includes("skip")) {
          data.companyName = input.trim();
        }
        nextStep = "address";
        response = `Client information collected. Now let's get the project details. What's the address for this project?`;
        break;
        
      case "address":
        data.address = input.trim();
        data.title = input.trim(); // Use address as title default
        nextStep = "title";
        response = `Got it. The address is ${data.address}. Would you like to provide a different title, or should I use the address as the title?`;
        break;
        
      case "title":
        if (input.toLowerCase().includes("use address") || input.toLowerCase().includes("same") || input.toLowerCase().includes("keep")) {
          // Keep address as title
        } else {
          data.title = input.trim();
        }
        nextStep = "architect";
        response = `Title set. Who is the architect for this project? (Say "skip" if not applicable)`;
        break;
        
      case "architect":
        if (!input.toLowerCase().includes("skip")) {
          data.architect = input.trim();
        }
        nextStep = "sqFt";
        response = `Architect noted. What's the square footage of the project? (Say "skip" if not applicable)`;
        break;
        
      case "sqFt":
        if (!input.toLowerCase().includes("skip")) {
          const sqFtMatch = input.match(/\d+/);
          if (sqFtMatch) {
            data.sqFt = sqFtMatch[0];
          }
        }
        nextStep = "units";
        response = `Square footage recorded. How many units does this project have? (Say "skip" if not applicable)`;
        break;
        
      case "units":
        if (!input.toLowerCase().includes("skip")) {
          const unitsMatch = input.match(/\d+/);
          if (unitsMatch) {
            data.units = unitsMatch[0];
          }
        }
        nextStep = "newConstruction";
        response = `Units recorded. Is this new construction? (Say "yes" or "no")`;
        break;
        
      case "newConstruction":
        data.newConstruction = input.toLowerCase().includes("yes") || input.toLowerCase().includes("y");
        nextStep = "building";
        response = `New construction: ${data.newConstruction ? "Yes" : "No"}. What type of building is this? You can choose from: Residential, Mixed use, Mercantile, Commercial, Storage, Warehouse, or Institutional. (You can select multiple, say "skip" if not applicable)`;
        break;
        
      case "building":
        if (!input.toLowerCase().includes("skip")) {
          const buildingTypes = ["Residential", "Mixed use", "Mercantile", "Commercial", "Storage", "Warehouse", "Institutional"];
          const selected = buildingTypes.filter(type => 
            input.toLowerCase().includes(type.toLowerCase())
          );
          if (selected.length > 0) {
            data.building = selected;
          }
        }
        nextStep = "project";
        response = `Building type recorded. What type of project is this? You can choose from: Sprinkler, Alarm, Mechanical, Electrical, Plumbing, Civil engineering, or Other. (You can select multiple, say "skip" if not applicable)`;
        break;
        
      case "project":
        if (!input.toLowerCase().includes("skip")) {
          const projectTypes = ["Sprinkler", "Alarm", "Mechanical", "Electrical", "Plumbing", "Civil engineering", "Other"];
          const selected = projectTypes.filter(type => 
            input.toLowerCase().includes(type.toLowerCase())
          );
          if (selected.length > 0) {
            data.project = selected;
          }
        }
        nextStep = "tier";
        response = `Project type recorded. What tier is this? You can choose: Tier I, Tier II, or Tier III. (Say "skip" if not applicable)`;
        break;
        
      case "tier":
        if (!input.toLowerCase().includes("skip")) {
          if (input.toLowerCase().includes("tier i") || input.toLowerCase().includes("tier 1")) {
            data.tier = ["Tier I"];
          } else if (input.toLowerCase().includes("tier ii") || input.toLowerCase().includes("tier 2")) {
            data.tier = ["Tier II"];
          } else if (input.toLowerCase().includes("tier iii") || input.toLowerCase().includes("tier 3")) {
            data.tier = ["Tier III"];
          }
        }
        nextStep = "service";
        response = `Tier recorded. What's the supply or service type? Options are: Pump & Tank 300, Pump & Tank 600, 2' Copper, 4' Ductile, 6' Ductile, or Unknown. (Say "skip" if not applicable)`;
        break;
        
      case "service":
        if (!input.toLowerCase().includes("skip")) {
          const serviceTypes = ["Pump & Tank 300", "Pump & Tank 600", "Pump & Tank", "2' Copper", "4' Ductile", "6' Ductile", "Unknown"];
          const selected = serviceTypes.find(type => 
            input.toLowerCase().includes(type.toLowerCase().replace("'", ""))
          );
          if (selected) {
            data.service = selected === "Pump & Tank" ? "Pump & Tank 300" : selected;
          }
        }
        nextStep = "nfpaVersion";
        response = `Service type recorded. What NFPA version applies? Options are: 13, 13R, or 13D. (Say "skip" if not applicable)`;
        break;
        
      case "nfpaVersion":
        if (!input.toLowerCase().includes("skip")) {
          if (input.toLowerCase().includes("13r")) {
            data.nfpaVersion = "13R";
          } else if (input.toLowerCase().includes("13d")) {
            data.nfpaVersion = "13D";
          } else if (input.match(/\b13\b/)) {
            data.nfpaVersion = "13";
          }
        }
        nextStep = "requestedDocs";
        response = `NFPA version recorded. What reports are required? Options include: Narrative, Sprinkler, Alarm, Hydraulic Calculations, Fire Hydrant Flow Test, NFPA 241, IEBC, or IBC. (You can select multiple, say "skip" if not applicable)`;
        break;
        
      case "requestedDocs":
        if (!input.toLowerCase().includes("skip")) {
          const docTypes = ["Narrative", "Sprinkler", "Alarm", "Hydraulic Calculations", "Fire Hydrant Flow Test", "NFPA 241", "IEBC", "IBC"];
          const selected = docTypes.filter(type => 
            input.toLowerCase().includes(type.toLowerCase())
          );
          if (selected.length > 0) {
            data.requestedDocs = selected;
          } else {
            // Default to common ones
            data.requestedDocs = ["Narrative", "Sprinkler", "Hydraulic Calculations", "Fire Hydrant Flow Test"];
          }
        } else {
          // Default to common ones
          data.requestedDocs = ["Narrative", "Sprinkler", "Hydraulic Calculations", "Fire Hydrant Flow Test"];
        }
        nextStep = "description";
        response = `Reports required recorded. Would you like to add a description for this project? (Say "skip" if not needed)`;
        break;
        
      case "description":
        if (!input.toLowerCase().includes("skip")) {
          data.description = input.trim();
        }
        nextStep = "siteAccess";
        response = `Description added. What's the site access information? (Say "skip" if not applicable)`;
        break;
        
      case "siteAccess":
        if (!input.toLowerCase().includes("skip")) {
          data.siteAccess = input.trim();
        }
        nextStep = "commencementOfConstruction";
        response = `Site access recorded. What's the estimated commencement of construction? (Say "skip" if not applicable)`;
        break;
        
      case "commencementOfConstruction":
        if (!input.toLowerCase().includes("skip")) {
          data.commencementOfConstruction = input.trim();
        }
        nextStep = "buildingHeight";
        response = `Commencement date recorded. What's the building height? (Say "skip" if not applicable)`;
        break;
        
      case "buildingHeight":
        if (!input.toLowerCase().includes("skip")) {
          const heightMatch = input.match(/\d+/);
          if (heightMatch) {
            data.buildingHeight = heightMatch[0];
          }
        }
        nextStep = "floorsBelowGrade";
        response = `Building height recorded. How many floors below grade? (Say "skip" if not applicable)`;
        break;
        
      case "floorsBelowGrade":
        if (!input.toLowerCase().includes("skip")) {
          const floorsMatch = input.match(/\d+/);
          if (floorsMatch) {
            data.floorsBelowGrade = floorsMatch[0];
          }
        }
        nextStep = "complete";
        response = `All information collected. Creating your project now...`;
        break;
        
      case "complete":
        // Shouldn't reach here, but handle it
        return;
    }
    
        // Add assistant response
    if (!response) {
      VoiceAssistantLogger.error("project", "No response generated for step", { step, input });
      return;
    }
    
    VoiceAssistantLogger.info("project", "Adding assistant response", { 
      step, 
      nextStep, 
      responsePreview: response.substring(0, 50) + "...",
    });
    
    addMessage("assistant", response);
    
    // Update state BEFORE speaking to prevent race conditions
    if (nextStep === "complete") {
      VoiceAssistantLogger.info("project", "All project fields collected, creating project", { 
        dataKeys: Object.keys(data),
        hasAddress: !!data.address,
        hasAuthorId: !!data.authorId,
      });
      projectCreationState.step = "complete";
      updateStatus("Creating project...", "bg-purple-400");
      // Create the project
      await createProjectFromVoice(data);
    } else if (nextStep) {
      VoiceAssistantLogger.info("project", "Moving to next step", { 
        from: step, 
        to: nextStep,
        dataKeys: Object.keys(data),
        currentState: projectCreationState,
      });
      
      // Update the step in projectCreationState
      projectCreationState.step = nextStep;
      
      // Update status with readable step name
      const stepNames = {
        searchClient: "Searching for client",
        selectClient: "Selecting client",
        firstName: "Client first name",
        lastName: "Client last name",
        email: "Client email",
        companyName: "Client company",
        address: "Project address",
        title: "Project title",
        architect: "Architect",
        sqFt: "Square footage",
        units: "Units",
        newConstruction: "New construction",
        building: "Building type",
        project: "Project type",
        tier: "Tier",
        service: "Service type",
        nfpaVersion: "NFPA version",
        requestedDocs: "Reports required",
        description: "Description",
        siteAccess: "Site access",
        commencementOfConstruction: "Commencement date",
        buildingHeight: "Building height",
        floorsBelowGrade: "Floors below grade",
      };
      
      const statusText = stepNames[nextStep] || nextStep;
      updateStatus(`Creating project - ${statusText}`, "bg-blue-400");
      
      // Add a cooldown period after asking a question to prevent immediate re-processing
      // Reset the last processed transcript so the next response will be accepted
      lastProcessedTranscript = "";
      lastProcessedTime = 0;
      
      // Speak response after a brief delay to ensure state is updated
      if (enableSpeechCheckbox.checked) {
        // Wait a moment before speaking to allow user to finish
        setTimeout(() => {
          speak(response);
        }, 800);
      }
    } else {
      VoiceAssistantLogger.error("project", "No nextStep defined", { step, input, response });
    }
  }
  
  // Search for clients by name, company, or email
  async function searchClients(query) {
    VoiceAssistantLogger.info("project", "Searching for clients", { query });
    try {
      const response = await fetch(`/api/users/get?role=Client&search=${encodeURIComponent(query)}&limit=10`, {
        method: "GET",
        headers: {
          "Content-Type": "application/json",
        },
      });
      
      if (!response.ok) {
        VoiceAssistantLogger.error("project", "Client search failed", { 
          status: response.status, 
          statusText: response.statusText 
        });
        console.error("Error searching clients:", response.statusText);
        return [];
      }
      
      const data = await response.json();
      const results = data.data || [];
      VoiceAssistantLogger.info("project", "Client search completed", { 
        query, 
        resultCount: results.length 
      });
      return results;
    } catch (error) {
      VoiceAssistantLogger.error("project", "Error searching clients", { 
        error: error.message, 
        stack: error.stack,
        query 
      });
      console.error("Error searching clients:", error);
      return [];
    }
  }

  // Create project from collected voice data
  async function createProjectFromVoice(projectData) {
    VoiceAssistantLogger.info("project", "Creating project from voice data", { 
      projectDataKeys: Object.keys(projectData),
      hasAuthorId: !!projectData.authorId,
      hasAddress: !!projectData.address,
    });
    updateStatus("Creating project...", "bg-purple-400");
    
    try {
      // Ensure arrays are arrays
      if (projectData.building && !Array.isArray(projectData.building)) {
        projectData.building = [projectData.building];
      }
      if (projectData.project && !Array.isArray(projectData.project)) {
        projectData.project = [projectData.project];
      }
      if (projectData.tier && !Array.isArray(projectData.tier)) {
        projectData.tier = [projectData.tier];
      }
      if (projectData.requestedDocs && !Array.isArray(projectData.requestedDocs)) {
        projectData.requestedDocs = [projectData.requestedDocs];
      }
      
      // Convert numeric strings to numbers
      if (projectData.sqFt) projectData.sqFt = parseInt(projectData.sqFt);
      if (projectData.units) projectData.units = parseInt(projectData.units);
      if (projectData.buildingHeight) projectData.buildingHeight = parseInt(projectData.buildingHeight);
      if (projectData.floorsBelowGrade) projectData.floorsBelowGrade = parseInt(projectData.floorsBelowGrade);
      
      VoiceAssistantLogger.info("api", "Calling projects/upsert API", { 
        method: "POST",
        dataKeys: Object.keys(projectData),
      });
      
      const response = await fetch("/api/projects/upsert", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(projectData),
      });
      
      VoiceAssistantLogger.info("api", "Received projects/upsert response", { 
        status: response.status,
        ok: response.ok,
      });
      
      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        VoiceAssistantLogger.error("api", "Project creation API error", { 
          status: response.status,
          errorData,
        });
        throw new Error(errorData.error || `Failed to create project: ${response.statusText}`);
      }
      
      const result = await response.json();
      
      if (result.success) {
        VoiceAssistantLogger.info("project", "Project created successfully", { 
          projectId: result.project?.id,
          title: result.project?.title,
          address: result.project?.address,
        });
        
        const successMsg = `Great! I've successfully created your project "${result.project.title || result.project.address}". The project ID is ${result.project.id}.`;
        addMessage("assistant", successMsg);
        if (enableSpeechCheckbox.checked) {
          speak(successMsg);
        }
        updateStatus("Project created!", "bg-green-400");
        
        // Reset project creation state
        projectCreationState = null;
        isAwaitingProjectField = false;
        
        if (!skipWakeWordCheckbox.checked) {
          setTimeout(() => {
            isAwake = false;
          }, 3000);
        }
      } else {
        VoiceAssistantLogger.error("project", "Project creation failed", { 
          error: result.error,
          result,
        });
        throw new Error(result.error || "Failed to create project");
      }
    } catch (error) {
      VoiceAssistantLogger.error("project", "Error creating project", { 
        error: error.message,
        stack: error.stack,
        projectData: projectData,
      });
      console.error("Error creating project:", error);
      const errorMsg = `Sorry, I encountered an error creating the project: ${error.message || "Please try again."}`;
      addMessage("assistant", errorMsg);
      updateStatus("Error creating project", "bg-red-400");
      if (enableSpeechCheckbox.checked) {
        speak(errorMsg);
      }
      
      // Reset project creation state on error
      projectCreationState = null;
      isAwaitingProjectField = false;
      
      if (!skipWakeWordCheckbox.checked) {
        setTimeout(() => {
          isAwake = false;
        }, 3000);
      }
    }
  }

  // Process command via API
  async function processCommand(text) {
    console.log("Sending to API:", { text, historyLength: conversationHistory.length });
    
    const response = await fetch("/api/voice-assistant/chat", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        message: text,
        conversationHistory: conversationHistory.slice(-10), // Last 10 exchanges
      }),
    });

    console.log("API response status:", response.status);

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.error || `API error: ${response.statusText}`);
    }

    const data = await response.json();
    console.log("API response data:", data);
    
    if (data.error) {
      throw new Error(data.error);
    }
    
    return data.response || "I'm not sure how to respond to that.";
  }

  // Add tag to current tags and update UI
  function addTag(tag) {
    const normalizedTag = tag.toLowerCase().trim();
    if (normalizedTag && !currentTags.includes(normalizedTag)) {
      currentTags.push(normalizedTag);
      updateTagsDisplay();
    }
  }

  // Remove tag from current tags
  function removeTag(tag) {
    currentTags = currentTags.filter(t => t !== tag);
    updateTagsDisplay();
  }

  // Update tags display in editor
  function updateTagsDisplay() {
    tagsContainer.innerHTML = "";
    
    if (currentTags.length === 0) {
      const emptyMsg = document.createElement("div");
      emptyMsg.className = "text-sm text-gray-400 italic";
      emptyMsg.textContent = "No tags yet. Add tags above or they'll be auto-extracted.";
      tagsContainer.appendChild(emptyMsg);
      return;
    }

    currentTags.forEach(tag => {
      const tagDiv = document.createElement("div");
      tagDiv.className = "inline-flex items-center gap-1 px-2 py-1 bg-blue-100 dark:bg-blue-900 text-blue-800 dark:text-blue-200 rounded text-sm";
      
      const tagSpan = document.createElement("span");
      tagSpan.textContent = `#${tag}`;
      
      const removeBtn = document.createElement("button");
      removeBtn.className = "ml-1 text-blue-600 dark:text-blue-300 hover:text-blue-800 dark:hover:text-blue-100 font-bold";
      removeBtn.textContent = "√ó";
      removeBtn.title = "Remove tag";
      removeBtn.onclick = () => removeTag(tag);
      
      tagDiv.appendChild(tagSpan);
      tagDiv.appendChild(removeBtn);
      tagsContainer.appendChild(tagDiv);
    });
  }

  // Add message to conversation
  function addMessage(role, text, tags = []) {
    conversationHistory.push({ role, content: text });

    // Add tags to current tags if provided
    if (tags && tags.length > 0) {
      tags.forEach(tag => addTag(tag));
      // Show tags editor if tags are present
      if (tagsEditor) {
        tagsEditor.classList.remove("hidden");
      }
    }

    // Clear placeholder
    if (conversation.querySelector(".text-gray-500")) {
      conversation.innerHTML = "";
    }

    const messageDiv = document.createElement("div");
    messageDiv.className = `flex ${role === "user" ? "justify-end" : "justify-start"} mb-2`;

    const bubble = document.createElement("div");
    bubble.className = `max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
      role === "user"
        ? "bg-primary-600 text-white"
        : role === "system"
        ? "bg-yellow-100 dark:bg-yellow-900 text-yellow-800 dark:text-yellow-200 border border-yellow-300 dark:border-yellow-700"
        : "bg-gray-200 dark:bg-gray-700 text-gray-900 dark:text-white"
    }`;

    bubble.textContent = text;
    
    // Add tags display if present (read-only in conversation)
    if (tags && tags.length > 0) {
      const tagsDiv = document.createElement("div");
      tagsDiv.className = "mt-2 flex flex-wrap gap-1";
      tags.forEach(tag => {
        const tagSpan = document.createElement("span");
        tagSpan.className = "text-xs px-2 py-1 bg-blue-100 dark:bg-blue-900 text-blue-800 dark:text-blue-200 rounded";
        tagSpan.textContent = `#${tag}`;
        tagsDiv.appendChild(tagSpan);
      });
      bubble.appendChild(tagsDiv);
    }
    
    messageDiv.appendChild(bubble);
    conversation.appendChild(messageDiv);
    conversation.scrollTop = conversation.scrollHeight;
  }
  
  // Extract and display tags for user messages
  async function processMessageWithTags(role, text) {
    if (role === "user") {
      // Extract tags from user message
      const tags = await extractTags(text);
      if (tags.length > 0) {
        addMessage(role, text, tags);
        // Show tags in debug
        debugText.textContent += ` | Tags: ${tags.join(", ")}`;
      } else {
        addMessage(role, text);
      }
    } else {
      addMessage(role, text);
    }
  }

  // Update status
  function updateStatus(text, indicatorColor) {
    statusText.textContent = text;
    statusIndicator.className = `w-3 h-3 rounded-full ${indicatorColor}`;
  }

  // Get preferred voice (prefer female voices, fallback to any available)
  function getPreferredVoice() {
    if (!("speechSynthesis" in window)) return null;
    
    const voices = window.speechSynthesis.getVoices();
    if (voices.length === 0) return null;
    
    // Preferred voices (female, natural-sounding)
    const preferredVoiceNames = [
      "Samantha", // macOS female voice
      "Victoria", // macOS female voice
      "Karen", // macOS female voice
      "Google UK English Female",
      "Microsoft Zira - English (United States)",
      "Microsoft Hazel - English (Great Britain)",
    ];
    
    // Try to find a preferred voice
    for (const preferredName of preferredVoiceNames) {
      const voice = voices.find(v => 
        v.name.includes(preferredName) || 
        v.name.toLowerCase().includes(preferredName.toLowerCase())
      );
      if (voice) return voice;
    }
    
    // Fallback: find any female voice
    const femaleVoice = voices.find(v => 
      v.name.toLowerCase().includes("female") ||
      v.name.includes("Samantha") ||
      v.name.includes("Victoria") ||
      v.name.includes("Karen") ||
      v.name.includes("Zira") ||
      v.name.includes("Hazel")
    );
    if (femaleVoice) return femaleVoice;
    
    // Last resort: use default voice
    return voices[0];
  }

  // Speak text
  function speak(text) {
    if ("speechSynthesis" in window) {
      VoiceAssistantLogger.debug("voice", "Speaking text", { 
        textLength: text.length,
        textPreview: text.substring(0, 50) + (text.length > 50 ? "..." : ""),
      });
      
      // Stop any current speech
      stopSpeaking();
      
      const utterance = new SpeechSynthesisUtterance(text);
      
      // Set slower rate (0.7 = 70% speed, slower than normal)
      utterance.rate = 0.75;
      
      // Slightly lower pitch for a warmer tone
      utterance.pitch = 0.9;
      
      // Normal volume
      utterance.volume = 1.0;
      
      // Set preferred voice
      const preferredVoice = getPreferredVoice();
      if (preferredVoice) {
        utterance.voice = preferredVoice;
        VoiceAssistantLogger.debug("voice", "Using voice", { voiceName: preferredVoice.name });
      }
      
      utterance.onstart = () => {
        VoiceAssistantLogger.debug("voice", "Speech started");
      };
      
      utterance.onend = () => {
        VoiceAssistantLogger.debug("voice", "Speech ended");
      };
      
      utterance.onerror = (event) => {
        VoiceAssistantLogger.error("voice", "Speech synthesis error", { 
          error: event.error,
          type: event.type,
        });
      };
      
      currentSpeechUtterance = utterance;
      window.speechSynthesis.speak(utterance);
    } else {
      VoiceAssistantLogger.warn("voice", "Speech synthesis not available");
    }
  }
  
  // Load voices when they become available (some browsers need this)
  if ("speechSynthesis" in window) {
    // Load voices immediately if available
    if (window.speechSynthesis.getVoices().length > 0) {
      // Voices already loaded
    } else {
      // Wait for voices to load
      window.speechSynthesis.onvoiceschanged = () => {
        console.log("Voices loaded:", window.speechSynthesis.getVoices().length);
      };
    }
  }

  // Stop speaking
  function stopSpeaking() {
    if ("speechSynthesis" in window) {
      window.speechSynthesis.cancel();
      currentSpeechUtterance = null;
    }
  }

  // Extract tags from text using AI
  async function extractTags(text) {
    try {
      const response = await fetch("/api/voice-assistant/extract-tags", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ text }),
      });

      if (!response.ok) {
        console.error("Tag extraction failed");
        return [];
      }

      const data = await response.json();
      return data.tags || [];
    } catch (error) {
      console.error("Error extracting tags:", error);
      return [];
    }
  }

  // Save conversation to learning table
  async function saveConversation() {
    if (conversationHistory.length === 0) {
      console.log("No conversation to save");
      return false;
    }

    // Get the last exchange (user question + assistant response)
    const lastExchange = conversationHistory.slice(-2);
    if (lastExchange.length < 2) {
      console.log("Not enough conversation to save");
      return false;
    }

    const userMessage = lastExchange.find(m => m.role === "user")?.content || "";
    const assistantMessage = lastExchange.find(m => m.role === "assistant")?.content || "";

    if (!userMessage || !assistantMessage) {
      console.log("Missing user or assistant message");
      return false;
    }

    // Create a knowledge entry from the conversation
    const title = userMessage.substring(0, 100) + (userMessage.length > 100 ? "..." : "");
    const content = `User asked: "${userMessage}"\n\nAssistant responded: "${assistantMessage}"`;

    // Use current tags (from manual entry + auto-extraction), or extract if none (only if enabled)
    let tagsToSave = [...currentTags];
    
    if (tagsToSave.length === 0 && enableTagsCheckbox?.checked) {
      // Auto-extract tags if none manually added and tags are enabled
      const combinedText = `${userMessage} ${assistantMessage}`;
      const extractedTags = await extractTags(combinedText);
      tagsToSave = extractedTags;
      // Add extracted tags to current tags for display
      extractedTags.forEach(tag => addTag(tag));
    }
    
    // Get priority from dropdown
    const priority = parseInt(prioritySelect?.value || "0", 10);
    currentPriority = priority;
    
    console.log("Saving with tags:", tagsToSave, "and priority:", priority);

    try {
      const response = await fetch("/api/voice-assistant/remember", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          title,
          content,
          category: "conversation_memory",
          tags: tagsToSave, // Use current tags (manual + auto-extracted)
          priority: priority, // Include priority
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error || `API error: ${response.statusText}`);
      }

      const data = await response.json();
      console.log("Conversation saved with tags and priority:", data);
      
      // Show confirmation with tags and priority
      const priorityLabels = { 0: "Normal", 1: "Low", 5: "Medium", 10: "High", 20: "Critical" };
      const priorityLabel = priorityLabels[priority] || `Priority ${priority}`;
      
      if (tagsToSave.length > 0) {
        const tagsDisplay = tagsToSave.map(tag => `#${tag}`).join(" ");
        addMessage("system", `Conversation saved with tags: ${tagsDisplay} (Priority: ${priorityLabel})`);
      } else {
        addMessage("system", `Conversation saved (Priority: ${priorityLabel})`);
      }
      
      // Clear tags and reset priority after saving
      currentTags = [];
      currentPriority = 0;
      updateTagsDisplay();
      if (prioritySelect) {
        prioritySelect.value = "0";
      }
      
      return true;
    } catch (error) {
      console.error("Error saving conversation:", error);
      return false;
    }
  }

  // Start audio recording for cloud transcription
  async function startAudioRecording() {
    try {
      audioStream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000
        } 
      });
      
      mediaRecorder = new MediaRecorder(audioStream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      audioChunks = [];
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data);
        }
      };
      
      // Handle when recording stops (for cloud transcription)
      mediaRecorder.onstop = async () => {
        if (audioChunks.length > 0) {
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          audioChunks = [];
          
          // Stop all tracks
          if (audioStream) {
            audioStream.getTracks().forEach(track => track.stop());
            audioStream = null;
          }
          
          // Transcribe with cloud API
          updateStatus("Transcribing...", "bg-purple-400");
          const cloudTranscript = await transcribeWithCloud(audioBlob);
          
          if (cloudTranscript && cloudTranscript.length > 0) {
            const correctedTranscript = correctCommonErrors(cloudTranscript);
            VoiceAssistantLogger.info("transcription", "Processing cloud transcript", {
              originalLength: cloudTranscript.length,
              correctedLength: correctedTranscript.length,
            });
            await processTranscript(correctedTranscript);
          } else {
            // Cloud transcription failed - show warning and auto-disable
            VoiceAssistantLogger.warn("transcription", "Cloud transcription returned empty or failed");
            addMessage("system", "‚ö†Ô∏è Cloud transcription unavailable. Auto-disabling cloud transcription. Browser speech recognition is now active.", []);
            updateStatus("Transcription failed - Using browser recognition", "bg-yellow-400");
            
            // Auto-disable cloud transcription if it fails
            if (useCloudTranscriptionCheckbox) {
              useCloudTranscriptionCheckbox.checked = false;
              VoiceAssistantLogger.info("system", "Auto-disabled cloud transcription due to failure");
            }
          }
          
          // Restart recording for next command if still listening
          if (isListening && useCloudTranscriptionCheckbox?.checked) {
            await startAudioRecording();
          }
        }
      };
      
      mediaRecorder.start(100); // Collect data every 100ms
    } catch (error) {
      console.error("Error starting audio recording:", error);
    }
  }

  // Stop audio recording (triggers onstop handler)
  function stopAudioRecording() {
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop();
    }
  }

  // Transcribe using cloud API
  async function transcribeWithCloud(audioBlob) {
    VoiceAssistantLogger.info("transcription", "Starting cloud transcription", {
      audioSize: audioBlob.size,
      audioType: audioBlob.type,
    });
    
    try {
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');
      
      const response = await fetch('/api/voice-assistant/transcribe', {
        method: 'POST',
        body: formData
      });
      
      VoiceAssistantLogger.debug("transcription", "Transcription API response", {
        status: response.status,
        ok: response.ok,
        statusText: response.statusText,
      });
      
      if (!response.ok) {
        const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
        VoiceAssistantLogger.error("transcription", "Cloud transcription API failed", {
          status: response.status,
          statusText: response.statusText,
          errorData,
        });
        
        // Provide helpful error message based on status
        if (response.status === 500) {
          throw new Error('Transcription service not configured. Please configure DEEPGRAM_API_KEY or GOOGLE_CLOUD_SPEECH_API_KEY, or disable cloud transcription.');
        } else if (response.status === 400) {
          throw new Error('Invalid audio format. Please try again.');
        } else {
          throw new Error(`Transcription failed: ${errorData.error || response.statusText}`);
        }
      }
      
      const data = await response.json();
      const transcript = data.transcript || '';
      
      VoiceAssistantLogger.info("transcription", "Cloud transcription successful", {
        transcriptLength: transcript.length,
        provider: data.provider || 'unknown',
        transcriptPreview: transcript.substring(0, 50) + (transcript.length > 50 ? '...' : ''),
      });
      
      return transcript;
    } catch (error) {
      VoiceAssistantLogger.error("transcription", "Cloud transcription error", {
        error: error.message,
        stack: error.stack,
        audioSize: audioBlob.size,
      });
      console.error('Cloud transcription error:', error);
      
      // Show user-friendly error message
      addMessage("system", `‚ö†Ô∏è Cloud transcription failed: ${error.message}. Falling back to browser speech recognition.`, []);
      
      return null; // Return null to trigger fallback
    }
  }

  // Toggle listening
  micButton.addEventListener("click", async () => {
    if (!recognition) return;

    if (isListening) {
      recognition.stop();
      isAwake = false;
      
      // Stop audio recording if using cloud transcription
      if (useCloudTranscriptionCheckbox?.checked && mediaRecorder) {
        stopAudioRecording();
      }
    } else {
      // Start audio recording if cloud transcription is enabled
      if (useCloudTranscriptionCheckbox?.checked) {
        await startAudioRecording();
      }
      
      recognition.start();
    }
  });
  
  // Handle manual stop for cloud transcription (when user pauses)
  let silenceTimer = null;
  function resetSilenceTimer() {
    if (silenceTimer) clearTimeout(silenceTimer);
    
    if (useCloudTranscriptionCheckbox?.checked && mediaRecorder && mediaRecorder.state === 'recording') {
      // Stop recording after 2 seconds of silence (user finished speaking)
      silenceTimer = setTimeout(() => {
        if (mediaRecorder && mediaRecorder.state === 'recording') {
          stopAudioRecording();
        }
      }, 2000);
    }
  }
  
  // Reset silence timer on any speech activity
  if (recognition) {
    const originalOnResult = recognition.onresult;
    recognition.onresult = (event) => {
      resetSilenceTimer();
      if (originalOnResult) originalOnResult.call(recognition, event);
    };
  }

  // Post-process transcript to fix common errors
  function correctCommonErrors(text) {
    let corrected = text;
    
    // Common transcription errors and corrections
    const corrections = {
      // Wake word variations
      "be": "Bee",
      "b": "Bee",
      "bee": "Bee",
      
      // Common word errors
      "what's": "what is",
      "whats": "what is",
      "that's": "that is",
      "thats": "that is",
      "it's": "it is",
      "its": "it is",
      "you're": "you are",
      "youre": "you are",
      "I'm": "I am",
      "im": "I am",
      
      // Technical terms that are often misheard
      "nfpa": "NFPA",
      "n f p a": "NFPA",
      "n f p": "NFPA",
      "fire protection": "fire protection",
      "fire alarm": "fire alarm",
      "fire sprinkler": "fire sprinkler",
      
      // Number corrections
      "one": "1",
      "two": "2",
      "three": "3",
      "four": "4",
      "five": "5",
      
      // Common phrase corrections
      "remember this": "remember this",
      "remember that": "remember this",
      "stop talking": "stop",
      "stop speaking": "stop",
    };
    
    // Apply corrections (case-insensitive)
    Object.keys(corrections).forEach((error) => {
      const regex = new RegExp(`\\b${error}\\b`, "gi");
      corrected = corrected.replace(regex, corrections[error]);
    });
    
    // Fix double spaces
    corrected = corrected.replace(/\s+/g, " ");
    
    return corrected.trim();
  }

  // Update wake word
  wakeWordInput.addEventListener("change", (e) => {
    wakeWord = e.target.value.toLowerCase().trim();
  });

  // Stop button handler
  btnStop.addEventListener("click", () => {
    stopSpeaking();
    addMessage("assistant", "Stopped speaking.");
    updateStatus("Ready - Say wake word", "bg-gray-400");
    isAwake = false;
  });

  // Remember button handler
  btnRemember.addEventListener("click", async () => {
    updateStatus("Saving conversation...", "bg-purple-400");
    try {
      const saved = await saveConversation();
      if (saved) {
        addMessage("assistant", "I've saved this conversation to my memory.");
        updateStatus("Conversation saved!", "bg-green-400");
      } else {
        addMessage("assistant", "I couldn't save the conversation. Please check the console for errors.");
        updateStatus("Save failed", "bg-red-400");
      }
    } catch (error) {
      console.error("Error saving conversation:", error);
      addMessage("assistant", "I encountered an error saving the conversation.");
      updateStatus("Error saving", "bg-red-400");
    }
  });

  // Export logs button handler
  btnExportLogs.addEventListener("click", () => {
    VoiceAssistantLogger.info("export", "Exporting logs");
    VoiceAssistantLogger.exportLogs();
  });

  // Help button handler
  btnHelp.addEventListener("click", () => {
    VoiceAssistantLogger.info("command", "Help button clicked");
    const helpText = `Here's what I can do:

üé§ Voice Commands:
‚Ä¢ "Bee [your question]" - Ask me anything
‚Ä¢ "Bee stop" - Stop me from talking
‚Ä¢ "Bee remember this" - Save our conversation
‚Ä¢ "Bee new job" or "Bee new project" - Create a new project (I'll ask you for all the details)
  - For Admin/Staff: I'll first ask if you want to use an existing client or create a new one
  - For Clients: I'll start directly with project details

üí° Tips:
‚Ä¢ Speak clearly for best results
‚Ä¢ Enable cloud transcription for better accuracy
‚Ä¢ Use the buttons below if voice recognition fails

üîß Available Actions:
‚Ä¢ Stop Speaking - Interrupts my speech
‚Ä¢ Remember This - Saves conversation to memory
‚Ä¢ Help - Shows this message`;

    addMessage("assistant", helpText);
    if (enableSpeechCheckbox.checked) {
      speak("Here's what I can do. Use Bee followed by your question, or say Bee stop to stop me from talking, or Bee remember this to save our conversation, or Bee new job to create a new project.");
    }
  });

  // Add tag button handler
  addTagBtn.addEventListener("click", () => {
    const tagValue = tagInput.value.trim();
    if (tagValue) {
      addTag(tagValue);
      tagInput.value = "";
      // Show tags editor if hidden
      if (tagsEditor) {
        tagsEditor.classList.remove("hidden");
      }
    }
  });

  // Add tag on Enter key
  tagInput.addEventListener("keypress", (e) => {
    if (e.key === "Enter") {
      e.preventDefault();
      addTagBtn.click();
    }
  });

  // Clear conversation
  clearButton.addEventListener("click", () => {
    conversationHistory = [];
    currentTags = [];
    currentPriority = 0;
    projectCreationState = null;
    isAwaitingProjectField = false;
    conversation.innerHTML =
      '<div class="text-sm text-gray-500 dark:text-gray-400 text-center">Your conversation will appear here...</div>';
    updateTagsDisplay();
    if (prioritySelect) {
      prioritySelect.value = "0";
    }
    if (tagsEditor) {
      tagsEditor.classList.add("hidden");
    }
    isAwake = false;
    updateStatus("Ready - Click microphone to start", "bg-gray-400");
  });
</script>

<style>
  #conversation {
    scrollbar-width: thin;
    scrollbar-color: rgba(156, 163, 175, 0.5) transparent;
  }

  #conversation::-webkit-scrollbar {
    width: 6px;
  }

  #conversation::-webkit-scrollbar-track {
    background: transparent;
  }

  #conversation::-webkit-scrollbar-thumb {
    background-color: rgba(156, 163, 175, 0.5);
    border-radius: 3px;
  }
</style>

